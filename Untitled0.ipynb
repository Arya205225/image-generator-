{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOfkG1SqenFj3Yi5tfl1Z1j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qNsv7pOo4QNL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768982845437,"user_tz":-330,"elapsed":1254,"user":{"displayName":"Aryan Krish","userId":"09386142905008930880"}},"outputId":"d5eb7f94-7b1b-4e3a-8405-1b1e5344059f"},"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: line 1: nvidia-smi: command not found\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","metadata":{"id":"86437873"},"source":["generator = StableDiffusionGenerator(model_id=\"runwayml/stable-diffusion-v1-9\", device=\"auto\")\n","\n","prompt = \"a photo of an astronaut riding a horse on mars, high quality, detailed\"\n","negative_prompt = \"blurry, low resolution, ugly, deformed\"\n","\n","try:\n","  image, metadata = generator.generate_image(\n","      prompt=prompt,\n","      negative_prompt=negative_prompt,\n","      width=512,\n","      height=512,\n","      num_inference_steps=25,\n","      guidance_scale=8.0,\n","      seed=42,\n","      scheduler=\"euler_a\"\n","  )\n","  display(image)\n","  print(\"Generated image metadata:\")\n","  for key, value in metadata.items():\n","    print(f\"  {key}: {value}\")\n","  # Optionally save the image\n","  # generator.save_image(image, metadata)\n","except Exception as e:\n","  print(f\"Error during image generation: {e}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","print(f'pythtorch version :{torch.__version__}')\n","print(f'cube version :{torch.version.cuda}')\n","if torch.cuda.is_available():\n","  print(f'cube device name:{torch.cuda.get_device_name(0)}')\n","  print(f'gpu name:{torch.cuda.get_device_name(0)}')\n","\n"],"metadata":{"id":"VtgdMw835OWN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/wh1/cu118"],"metadata":{"id":"22n0VC-v8N58"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install diffusers==0.21.0 transformers==4.30.2 accelerate==0.20.3"],"metadata":{"id":"UbVYH6FbleKX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install safetensors==0.3.1 xformers==0.0.20 Pillow==1.7.0"],"metadata":{"id":"ogeuGpzywWnM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install numpy==1.24.4 matplotli==3.7.2 gradio==4.0.0"],"metadata":{"id":"AVy04y3SxRtD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import torch\n","import torch.nn.functional as F"],"metadata":{"id":"MkNfZHP_xl3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from diffusers import StableDiffusionPipeline\n","import gradio as gr\n","from PIL import Image\n","import torch.nn.functional as F\n","from torch import autocast\n","import numpy as np\n","from PIL import Image\n","import os,time,gc\n","from typing import List,Tuple,Optional\n","from datetime import datetime\n","from diffusers import (\n","    StableDiffusionPipeline,\n","    EulerAncestralDiscreteScheduler,\n","    DPMSolverMultistepScheduler,\n","    EulerDiscreteScheduler,\n","    DDIMScheduler,\n","    DPMSolverMultistepScheduler,\n","    LMSDiscreteScheduler\n",")"],"metadata":{"id":"1PbfgLy9yFK_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'pytorch version: {torch.__version__}' )\n","print(f'CUDA available: {torch.cuda.is_available}')\n","print(f'GPU device name:{torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\"}')\n","\n","\n"],"metadata":{"id":"PfOOS7-GyjeN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import torch\n","import torch.nn.functional as F\n","\n","from diffusers import (\n","    StableDiffusionPipeline,\n","    EulerAncestralDiscreteScheduler,\n","    DPMSolverMultistepScheduler,\n","    EulerDiscreteScheduler,\n","    DDIMScheduler,\n","    LMSDiscreteScheduler\n",")\n","from PIL import Image\n","import os,time,gc\n","from typing import List,Tuple,Optional\n","from datetime import datetime\n","\n","class StableDiffusionGenerator:\n","  def __init__(self,model_id : str = \"runwayml/stable-diffusion-v1-5\",device:str = \"auto\"):\n","    try:\n","      self.device = self._setup_device(device)\n","      self.dtype = torch.float16 if self.device ==\"cuda\" else torch.float32\n","      print(f\"Initializing Stable Diffusion on{self.device}\")\n","      print(f\"Using precision:{self.dtype}\")\n","      self.pipe = self._load_pipeline(model_id)\n","      self.current_scheduler=\"euler_a\"\n","      self.schedulers= {\n","          \"euler_a\": (\"Euler Ancestral\",\"Fast, good for creative image\"),\n","          \"euler\": (\"Euler\",\"Deterministic,consistent results\"),\n","          \"ddim\":  (\"DDIM\",\"Classic,good quality,efficient\"),\n","          \"dpm_solver\":(\"DPM_Solver\",\"High quality, efficient\"),\n","          \"lms\": (\"LMSDiscreteScheduler\", \"Linear multistep, stable\")\n","      }\n","      print(\"Stable Diffusion initialized successfully\")\n","    except Exception as e:\n","      print(f\"Error initializing Stable Diffusion: {str(e)}\")\n","      raise\n","\n","  def _setup_device(self,device:str)-> torch.device:\n","    if device == \"auto\":\n","      if torch.cuda.is_available():\n","        device = \"cuda\"\n","        print(f\"Gpu Detected:{torch.cuda.get_device_name(0)}\")\n","        vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n","        print(f\"GPU VRAM:{vram_gb:.1f}GB\")\n","      else:\n","        device = \"cpu\"\n","        print(\"using cpu(Gpu not available)\")\n","      return torch.device(device)\n","\n","  def _load_pipeline(self,model_id:str)-> StableDiffusionPipeline:\n","    try:\n","      pipe = StableDiffusionPipeline.from_pretrained(\n","          model_id,\n","          torch_dtype=self.dtype,\n","          safety_checker = None,\n","          requires_safety_checker = False\n","        )\n","      print(\"Applying memory optimizations\")\n","      pipe.enable_attention_slicing()\n","      pipe.enable_vae_slicing()\n","      try:\n","        pipe.enable_xformers_memory_efficient_attention()\n","        print(\"Xformers Attention : Enabled\")\n","      except Exception as e:\n","        print(f\"xformers not available ({e})\")\n","\n","      if self.device.type == \"cuda\":\n","        try:\n","          pipe = pipe.to(self.device)\n","          print(\"full GPU Loading :Success\")\n","        except RuntimeError as e:\n","          print(\"GPU Memory Limited:Using CPU offload\")\n","          pipe.enable_model_cpu_offload()\n","      else:\n","        pipe.enable_sequential_cpu_offload()\n","        print(\"Cpu offload enabled\")\n","      return pipe\n","    except Exception as e:\n","         raise RuntimeError(f\"Error loading Stable Diffusion pipeline: {str(e)}\")\n","  def set_scheduler(self,scheduler_name:str)-> bool:\n","    if scheduler_name not in self.schedulers:\n","      print(f\"unknow scheduler:{scheduler_name}\")\n","      return False\n","    if scheduler_name == self.current_scheduler:\n","      return True\n","    scheduler_map = {\n","        \"euler_a\": EulerAncestralDiscreteScheduler,\n","        \"euler\": EulerDiscreteScheduler,\n","        \"ddim\": DDIMScheduler,\n","        \"dpm_solver\": DPMSolverMultistepScheduler,\n","        \"lms\": LMSDiscreteScheduler\n","     }\n","    try:\n","         scheduler_class = scheduler_map[scheduler_name]\n","         self.pipe.scheduler = scheduler_class.from_config(self.pipe.scheduler.config)\n","         self.current_scheduler = scheduler_name\n","         name, desc = self.schedulers[scheduler_name]\n","         print(f\"scheduler Changed:{name}({desc})\")\n","         return True\n","    except Exception as e:\n","         print(f\"scheduler Error:{(e)}\")\n","         return False\n","  def generate_image(\n","      self,\n","      prompt : str,\n","      negative_prompt:str = \"\",\n","      width:int = 512,\n","      height:int = 512,\n","      num_inference_steps:int =20 ,\n","      guidance_scale:float = 7.5,\n","      seed: Optional[int] = None,\n","      scheduler: str = \"euler_a\"\n","  ) -> Tuple[Image.Image,dict]:\n","      if not prompt.strip():\n","        raise ValueError(\"prompt cannot be empty\")\n","\n","      self.set_scheduler(scheduler)\n","      if seed is None:\n","        seed = torch.randint(0,2**32,(1,)).item()\n","\n","\n","      generator = torch.Generator(device=self.device)\n","      generator.manual_seed(seed)\n","      width = (width // 8) * 8\n","      height = (height // 8) * 8\n","      print(f\"generating:'{prompt[:50]}...' وصلت\")\n","      print(f\"size:{width}x{height},Steps: {num_inference_steps},CFG: {guidance_scale}\")\n","      print(f\"seed:{seed},scheduler : {scheduler}\")\n","\n","      start_time = time.time()\n","      try:\n","        with torch.inference_mode():\n","          if self.device.type == \"cuda\" and self.dtype == torch.float16:\n","            with autocast(\"cuda\"):\n","              result = self.pipe(\n","                  prompt= prompt,\n","                  height = height,\n","                  width = width,\n","                  negative_prompt = negative_prompt if negative_prompt else None,\n","                  num_inference_steps = num_inference_steps,\n","                  guidance_scale = guidance_scale,\n","                  generator = generator\n","              )\n","          else:\n","             result = self.pipe(\n","                  prompt= prompt,\n","                  height = height,\n","                  width = width,\n","                  negative_prompt = negative_prompt if negative_prompt else None,\n","                  num_inference_steps = num_inference_steps,\n","                  guidance_scale = guidance_scale,\n","                  generator = generator\n","              )\n","        generation_time = time.time() - start_time\n","        metadata = {\n","          \"prompt\":prompt,\n","          \"negative_prompt\":negative_prompt,\n","          \"width\":width,\n","          \"height\":height,\n","          \"num_inference_steps\":num_inference_steps,\n","          \"guidance_scale\":guidance_scale,\n","          \"seed\":seed,\n","          \"scheduler\":scheduler,\n","          \"generation_time\": round(generation_time,2),\n","          \"device\" : str(self.device),\n","          \"dtype\" : str(self.dtype)\n","        }\n","        print(f\"Generated in {generation_time:.2f}\")\n","        return result.images[0],metadata\n","      except torch.cuda.OutOfMemoryError:\n","        self._cleanup_memory()\n","        raise RuntimeError(\n","            \"GPU out of memory! try: reducing image size , fewer steps,\"\n","            \"or use CPU mode , Current settings may be too demanding.\"\n","         )\n","      except Exception as e:\n","         raise RuntimeError(f\" generation faild: {str(e)}\")\n","      finally:\n","         self._cleanup_memory()\n","\n","  def _cleanup_memory(self):\n","      gc.collect()\n","      if self.device.type == \"cuda\":\n","        torch.cuda.empty_cache()\n","  def get_memory_usage(self) -> dict :\n","    memory_info = {}\n","    if self.device.type == \"cuda\":\n","      memory_info = {\n","          \"allocated\": torch.cuda.memory_allocated() / 1024**3,\n","          \"reserved_gb\": torch.cuda.memory_reserved() / 1024**3,\n","          \"max_allocated_gb\": torch.cuda.max_memory_allocated() / 1024**3,\n","          \"total_gb\": torch.cuda.get_device_properties(0).total_memory / 1024**3\n","      }\n","    else:\n","      memory_info ={\"device\":\"cpu\",\"note\":\"CPU memory tracking not available\"}\n","    return memory_info\n","  def save_image(self,Image:Image.Image,metadata:dict,output_dir:str = \"output\") -> str:\n","    os.makedirs(output_dir,exist_ok=True)\n","    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    filename = f\"sd_gen_{timestamp}_s{metadata['seed']}_{metadata['width']}x{metadata['height']}.png\"\n","    filepath = os.path.join(output_dir,filename)\n","    Image.save(filepath)\n","\n","    metadata_file = filepath.replace(\".png\",\"_metadata.txt\")\n","    with open(metadata_file,\"w\") as f:\n","      f.write(\"Stable Diffusion Generation Metadata\\n\")\n","      f.write(\"=\" * 40 + \"\\n\")\n","      for key,value in metadata.items():\n","        f.write(f\"{key}:{value}\\n\")\n","    print(f\"saved:{filepath}\")\n","    return filepath"],"metadata":{"id":"L6YWUrpp3W81","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from logging import PlaceHolder\n","class StableDiffusionUI:\n","  def __init__(self):\n","    self.generator = None\n","    self.gallery_images = []\n","    self.generation_history = []\n","  def initialize_generator(self,model_choice: str,device_choice: str) -> str:\n","    try:\n","      model_map={\n","          \"Stable Diffusion 1.5 (Recommended)\": \"runwayml/stable-diffusion-v1-5\",\n","          \"Stable Diffusion 2.1\": \"stabilityai/stable-diffusion-2-1\",\n","          \"Realistic Vision (RealVisXL)\" : \"SG161222/Realistic_Vision_V4.0\"\n","          }\n","      device_map = {\n","          \"CPU (Slower)\": \"cpu\",\n","          \"GPU (CUDA)\": \"cuda\",\n","          \"AUTO (Recommended)\": \"auto\"\n","      }\n","      model_id = model_map.get(model_choice, \"runwayml/stable-diffusion-v1-5\")\n","      device = device_map.get(device_choice, \"auto\")\n","\n","      self.generator = StableDiffusionGenerator(model_id=model_id, device=device)\n","      memory_info = self.generator.get_memory_usage()\n","      memory_text = f\"memory Usage: {memory_info}\" if memory_info else \"Ready!\"\n","      return f\"model loaded successfully!\\n{memory_text}\"\n","    except Exception as e:\n","      return f\"Error initializing generator: {str(e)}\"\n","\n","  def generate_image(\n","      self,\n","      prompt: str,\n","      negative_prompt: str,\n","      width: int,\n","      height: int,\n","      steps: int,\n","      guidance: float,\n","      seed: int,\n","      scheduler: str,\n","      save_image: bool\n","  ) -> Tuple[Optional[Image.Image], str , str]:\n","      if self.generator is None:\n","        return None, \"Please initialize the generator first.\", \"\"\n","      if not prompt.strip():\n","        return None, \"Prompt cannot be empty.\", \"\"\n","      try:\n","        seed = None if seed == -1 else int(seed)\n","        image, metadata = self.generator.generate_image(\n","            prompt=prompt,\n","            negative_prompt=negative_prompt,\n","            width=width,\n","            height=height,\n","            num_inference_steps=steps,\n","            guidance_scale=guidance,\n","            seed=seed,\n","            scheduler=scheduler\n","        )\n","        info_text = self._format_generation_info(metadata)\n","        saved_path = \"\"\n","        if save_image:\n","          saved_path = self.generator.save_image(image, metadata)\n","        self.gallery_images.append(image)\n","        self.generation_history.append((prompt, metadata))\n","        if len(self.gallery_images) > 10:\n","          self.gallery_images = self.gallery_images[-10:]\n","          self.generation_history = self.generation_history[-10:]\n","\n","        return image, info_text, saved_path\n","      except Exception as e:\n","        return None, f\"Generation failed: {str(e)}\", \"\"\n","\n","  def _format_generation_info(self,metadata:dict)-> str:\n","    return f\"\"\"\n","Generation Complete!\n","\n","Parameters Used:\n","- Prompt: {metadata['prompt'][:100]}{'...' if len(metadata['prompt']) > 100 else ''}\n","- size: {metadata['width']}x{metadata['height']} pixels\n","- Steps: {metadata['num_inference_steps']} (more steps = higher quality , slower)\n","- Guidance Scale: {metadata['guidance_scale']} (higher= follows prompt more closely)\n","- Scheduler : {metadata['scheduler']}\n","- Seed :{metadata['seed']} (for reproducibe results)\n","\n","\n","performance:\n","- Generation Time : {metadata['generation_time']} s\n","- Device : {metadata['device']}\n","- dtype : {metadata['dtype']}\n","\"\"\"\n","  def get_example_prompts(self) -> list:\n","    return [\n","        [\"a serene mountain landscape at sunrise , photorealistic, highly detailed\",\"blurry,low quality\"],\n","        [\"protrait of a wise old wizard , fantasy art , digital painting\",\"ugly , deformed\"],\n","        [\"Cyberpunk at night , neon light,futuristic\", \"daytime,bright\"],\n","        [\"cute cartoon cat wearing  a hat , kawail style\",\"realstic , scary\"],\n","        [\"abstract geometric patterns , colorful , modern art \", \"representational , dull colors\"]\n","\n","    ]\n","\n","  def show_scheduler_info(self,scheduler:str)-> str:\n","    scheduler_info = {\n","        \"euler_a\": \"Euler Ancestral,Fast, good for creative image,adds slight randomness for variety\",\n","        \"euler\": \"Euler :Deterministic,consistent , same seed = same result\",\n","        \"ddim\":  \"DDIM :Classic,good quality,efficient but slower\",\n","        \"dpm_solver\" : \"DPM solver : Efficient high-quality generation \",\n","        \"lms\" : \"LMS : linear multistep , very stable results\"\n","    }\n","    return scheduler_info.get(scheduler,\"Scheduler information not available\")\n","  def get_memory_info(self) -> str:\n","    if self.generator is None:\n","      return \"model not loaded\"\n","    try:\n","      memory_info = self.generator.get_memory_usage()\n","      if 'allocated' in memory_info:\n","        return f\"\"\"\n","\n","GPU Memory Usage:\n","- Allocated: {memory_info['allocated']:.2f} GB\n","- Reserved: {memory_info['reserved_gb']:.2f} GB\n","- Total Available: {memory_info['total_gb']:.2f} GB\n","- Usage : {(memory_info ['allocated']/memory_info['total_gb']*100):.2f}%\n","                 \"\"\"\n","      else:\n","        return \"CPU mode - memory tracking not available\"\n","    except :\n","          return \"Memory info unavailable\"\n","\n","  def create_interface(self) -> gr.Blocks:\n","    with gr.Blocks(\n","        title = \"Education Stable Diffusion Generator\",\n","        theme=gr.themes.Soft()\n","       ) as interface:\n","            gr.Markdown (\"\"\"\n","            #Education Stable Diffusion Generator\n","            ** Learn Generative AI concepts while creating images!**\n","\n","          \"\"\")\n","            with gr.Tab(\"Setup & Generation \"):\n","              with gr.Row():\n","                with gr.Column():\n","                  gr.Markdown(\"## Model Setup\")\n","                  model_choice = gr.Dropdown(\n","                      choices = [\n","                          \"Stable Diffusion 1.5 (Recommended)\",\n","                          \"Stable Diffusion 2.1\",\n","                          \"Realistic Vision (RealVisXL)\"\n","                      ],\n","                      value = \"Stable Diffusion 1.5 (Recommended)\",\n","                      label = \"Model Selection\"\n","                  )\n","                  device_choice = gr.Dropdown(\n","                      choices = [\n","                          \"CPU (Slower)\",\n","                          \"GPU (CUDA)\",\n","                          \"AUTO (Recommended)\"\n","                      ],\n","                      value = \"AUTO (Recommended)\",\n","                      label = \"Device Selection\"\n","                  )\n","                  init_btn = gr.Button(\"Initialize Model\", variant=\"primary\")\n","                  init_status = gr.Textbox(\n","                      label = \"Initialization Status\",\n","                      placeholder=\"click Initialze model to start\",\n","                      lines = 3\n","                  )\n","                  with gr.Column():\n","                    gr.Markdown(\"### System Info\")\n","                    memory_btn = gr.Button (\"Check Memory Usage\")\n","                    memory_info = gr.Textbox(\n","                        label=\"Memory Information\",\n","                        placeholder=\" click to Check memory usage\",\n","                        lines=6\n","                    )\n","                    generate_btn = gr.Button(\"Generate Image\",variant=\"primary\",size=\"lg\")\n","                with gr.Column():\n","                  with gr.Accordion(\"Advanced Settings\",open=True):\n","                    with gr.Row():\n","                      width = gr.Slider(256,1024,512, step=64, label=\"width\")\n","                      height = gr.Slider(256,1024,512, step=64, label=\"height\")\n","                    with gr.Row():\n","                      steps=gr.Slider(10, 100, 20, step=1,label=\"Inference Steps\")\n","                      guidance = gr.Slider(1.0,20.0,7.5,step=0.5,label = \"Guidance Scale\")\n","                    scheduler = gr.Dropdown(\n","                        choices = [\"euler_a\",\"euler\",\"ddim\",\"dpm_solver\",\"lms\"],\n","                        value = \"euler_a\",\n","                        label = \"Scheduler\"\n","                    )\n","                    scheduler_info = gr.Textbox(\n","                        label = \"Scheduler Information\",\n","                        interactive=False,\n","                        lines = 2\n","                    )\n","                    with gr.Row():\n","                      seed = gr.Number(-1,label=\"Seed\")\n","                      save_image = gr.Checkbox(label=\"save Generated Image\")\n","              with gr.Row():\n","                output_image = gr.Image(label = \"Generated Image\",type=\"pil\")\n","                with gr.Column():\n","                  generation_info = gr.Textbox(\n","                      label =\"Generation Info\",\n","                      lines=10,\n","                      interactive=False\n","\n","                  )\n","                  saved_path = gr.Textbox (\n","                      label=\"saved file path\",\n","                      interactive=False\n","                  )\n","\n","            with gr.Tab(\"Examples & Gallery\"):\n","              gr.Markdown(\"### Examples prompt to try\")\n","              prompt = gr.Textbox(\"\", label=\"Prompt\") # Define prompt and negative_prompt here for examples\n","              negative_prompt = gr.Textbox(\"\", label=\"Negative Prompt\")\n","              examples = gr.Examples(\n","                  examples=self.get_example_prompts(),\n","                  inputs=[prompt,negative_prompt],\n","                  label=\"Click any example to load it\"\n","              )\n","              gr.Markdown(\"### Recent Generations\")\n","              gallery = gr.Gallery(\n","                  value=[],\n","                  label=\"your Generated  Image\",\n","                  show_label=True,\n","                  columns=3,\n","                  rows=2,\n","                  object_fit=\"contain\",\n","                  height=\"auto\"\n","              )\n","            init_btn.click(\n","              fn=self.initialize_generator,\n","              inputs=[model_choice,device_choice],\n","              outputs=[init_status]\n","           )\n","            generate_btn.click(\n","                fn=self.generate_image,\n","                inputs=[prompt,negative_prompt,width,height,steps,guidance,seed,scheduler,save_image],\n","                outputs=[output_image,generation_info,saved_path]\n","            ).then(\n","                fn =lambda:self.gallery_images,\n","                outputs=gallery\n","            )\n","            scheduler.change(\n","                fn=self.show_scheduler_info,\n","                inputs=scheduler,\n","                outputs=scheduler_info\n","            )\n","            memory_btn.click(\n","                fn=self.get_memory_info,\n","                outputs=memory_info\n","            )\n","    return interface"],"metadata":{"id":"21tXs6HRP2wC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ui = StableDiffusionUI()\n","interface = ui.create_interface()\n","interface.launch(\n","    share=True,\n","    server_name=\"0.0.0.0\",\n","    server_port=7860,\n","    debug=True,\n","    show_error=True\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"wse9wxL-TgVy","outputId":"9e500fc4-b99d-4772-bfac-ed0ef38b55df"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","* Running on public URL: https://173170f41c3fc94606.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"data":{"text/html":["<div><iframe src=\"https://173170f41c3fc94606.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Gpu Detected:Tesla T4\n","GPU VRAM:14.7GB\n","Initializing Stable Diffusion oncuda\n","Using precision:torch.float32\n","Error initializing Stable Diffusion: 'StableDiffusionGenerator' object has no attribute '_load_pipeline'\n","Gpu Detected:Tesla T4\n","GPU VRAM:14.7GB\n","Initializing Stable Diffusion oncuda\n","Using precision:torch.float32\n","Error initializing Stable Diffusion: 'StableDiffusionGenerator' object has no attribute '_load_pipeline'\n","Gpu Detected:Tesla T4\n","GPU VRAM:14.7GB\n","Initializing Stable Diffusion oncuda\n","Using precision:torch.float32\n","Error initializing Stable Diffusion: 'StableDiffusionGenerator' object has no attribute '_load_pipeline'\n","Gpu Detected:Tesla T4\n","GPU VRAM:14.7GB\n","Initializing Stable Diffusion oncuda\n","Using precision:torch.float32\n","Error initializing Stable Diffusion: 'StableDiffusionGenerator' object has no attribute '_load_pipeline'\n","Gpu Detected:Tesla T4\n","GPU VRAM:14.7GB\n","Initializing Stable Diffusion oncuda\n","Using precision:torch.float32\n","Error initializing Stable Diffusion: 'StableDiffusionGenerator' object has no attribute '_load_pipeline'\n","Gpu Detected:Tesla T4\n","GPU VRAM:14.7GB\n","Initializing Stable Diffusion oncuda\n","Using precision:torch.float32\n","Error initializing Stable Diffusion: 'StableDiffusionGenerator' object has no attribute '_load_pipeline'\n","Gpu Detected:Tesla T4\n","GPU VRAM:14.7GB\n","Initializing Stable Diffusion oncuda\n","Using precision:torch.float32\n","Error initializing Stable Diffusion: 'StableDiffusionGenerator' object has no attribute '_load_pipeline'\n"]}]}]}